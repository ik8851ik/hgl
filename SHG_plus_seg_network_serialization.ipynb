{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2# 한 iteration에 몇개의 batch를 사용할지\n",
    "image_resize_w = 256\n",
    "image_resize_h = 256\n",
    "nstack = 4  # num of hourglass stack\n",
    "\n",
    "net_ouput_w = 64\n",
    "net_ouput_h = 64\n",
    "\n",
    "iterations = 60000000 # 몇 iteration을 학습할지\n",
    "learning_rate = 0.0001\n",
    "result_save_freq = 10000 # result를 save하는 주기 (몇 iter마다 save할지)\n",
    "summary_dir = \"./log\"\n",
    "\n",
    "save_freq = 20000 # model weights를 저장하는 주기\n",
    "save_dir = \"./model\"\n",
    "\n",
    "vis_train_dir = \"./vis/train\"\n",
    "vis_val_dir = \"./vis/val\"\n",
    "\n",
    "#train_ratio = 0.8 # train의 비율, validation 비율 = 1-train_ratio\n",
    "validation_num = 500\n",
    "\n",
    "path_pose_joint = \"/media/vision/Seagate Expansion Drive/coco2014data/densepose_coco\" # 데이터셋의 경로\n",
    "\n",
    "\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if not os.path.exists(vis_train_dir):\n",
    "    os.makedirs(vis_train_dir)\n",
    "if not os.path.exists(vis_val_dir):\n",
    "    os.makedirs(vis_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_aug_fn(image, joints_image, segs_image):\n",
    "    h = len(image)\n",
    "    w = len(image[0])\n",
    "\n",
    "    joint = np.zeros([net_ouput_h,net_ouput_w,18], dtype=np.float32)  # 17 + background\n",
    "    joint_ori = np.zeros([net_ouput_h,net_ouput_w,18], dtype=np.float32)  # 17 + background\n",
    "    existing_joints = np.unique(joints_image)\n",
    "    \n",
    "    for existing_joint in existing_joints:\n",
    "        if existing_joint != 0:\n",
    "            tmp_y = np.where(joints_image == existing_joint)[0][0]\n",
    "            tmp_x = np.where(joints_image == existing_joint)[1][0]\n",
    "            resized_x = int(tmp_x/w * net_ouput_w)\n",
    "            resized_y = int(tmp_y/h * net_ouput_h)\n",
    "            joint[resized_y][resized_x][int(existing_joint/10)] = 1.0\n",
    "            joint_ori[resized_y][resized_x][int(existing_joint/10)] = 1.0\n",
    "    \n",
    "    for i in range(0,18):\n",
    "         joint[:,:,i] = keypoint_gauusian(joint[:,:,i])\n",
    "    \n",
    "    joint_bg = 1 - np.sum(joint[:,:,1:], axis=-1)\n",
    "    joint_bg[joint_bg < 0] = 0\n",
    "    joint[:,:,0] = joint_bg\n",
    "        \n",
    "    segs_image = np.squeeze(segs_image, axis=-1)\n",
    "    seg = np.eye(15,dtype=np.float32)[np.int32(segs_image/10)]\n",
    "    \n",
    "    \n",
    "    image = cv2.resize(image, (image_resize_h, image_resize_w))#, interpolation=cv2.INTER_NEAREST)\n",
    "    seg = cv2.resize(seg, (net_ouput_h, net_ouput_w), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return image, joint, seg, joint_ori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map_fn(frame_name, joint_name, seg_name):\n",
    "    image = tf.image.decode_jpeg(tf.read_file(frame_name), channels=3)\n",
    "    joints_image = tf.image.decode_jpeg(tf.read_file(joint_name), channels=1)\n",
    "    segs_image = tf.image.decode_png(tf.read_file(seg_name), channels=1)\n",
    "     \n",
    "    img, joint, seg, joint_ori  = tf.py_func(_data_aug_fn, [image, joints_image, segs_image], [tf.uint8, tf.float32, tf.float32, tf.float32])\n",
    "    \n",
    "    img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "   # img = img/255\n",
    "    \n",
    "    \n",
    "    return img, joint, seg, joint_ori\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densepose_agu(directory):\n",
    "    frame_name = []\n",
    "    joint_name = []\n",
    "    seg_name = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                tmp = []\n",
    "                frame_name.append(root + '/' + file)\n",
    "                joint_name.append(root + '/' + file[:-4] + '_keypoint.png')\n",
    "                seg_name.append(root + '/' + file[:-4] + '_seg.png')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((frame_name, joint_name, seg_name))\n",
    "    \n",
    "    ds_val = dataset.take(validation_num) \n",
    "    ds_train = dataset.skip(validation_num)\n",
    "\n",
    "    ds_train = ds_train.repeat()\n",
    "    ds_val = ds_val.repeat()\n",
    "\n",
    "    ds_train = ds_train.shuffle(buffer_size=4096)\n",
    "    ds_val = ds_val.shuffle(buffer_size=4096)\n",
    "        \n",
    "    ds_train = ds_train.map(_map_fn)\n",
    "    ds_val = ds_val.map(_map_fn)\n",
    "    \n",
    "    ds_train = ds_train.batch(batch_size)  \n",
    "    ds_val = ds_val.batch(batch_size) \n",
    "    \n",
    "    return ds_train, ds_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint_gauusian(single_keypoint_image):\n",
    "    if np.max(single_keypoint_image) == 0:\n",
    "        return single_keypoint_image\n",
    "    else:\n",
    "        h = len(single_keypoint_image)\n",
    "        w = len(single_keypoint_image[0])\n",
    "\n",
    "        ind = np.argmax(single_keypoint_image)\n",
    "\n",
    "        center_x = (ind) % w \n",
    "        center_y = (ind) // w \n",
    "\n",
    "\n",
    "      #  sigma = 0.5\n",
    "        sigma = 2\n",
    "        \n",
    "        th = 3.6052\n",
    "      #  th = 4.6052\n",
    "\n",
    "        delta = math.sqrt(th * 2)\n",
    "\n",
    "        x0 = int(max(0, center_x - delta * sigma + 0.5))\n",
    "        y0 = int(max(0, center_y - delta * sigma + 0.5))\n",
    "\n",
    "        x1 = int(min(w - 1, center_x + delta * sigma + 0.5))\n",
    "        y1 = int(min(h - 1, center_y + delta * sigma + 0.5))\n",
    "\n",
    "        exp_factor = 1 / 2.0 / sigma / sigma\n",
    "\n",
    "        ## fast - vectorize\n",
    "        arr_heatmap = single_keypoint_image[y0:y1 + 1, x0:x1 + 1]\n",
    "        y_vec = (np.arange(y0, y1 + 1) - center_y)**2  # y1 included\n",
    "        x_vec = (np.arange(x0, x1 + 1) - center_x)**2\n",
    "        xv, yv = np.meshgrid(x_vec, y_vec)\n",
    "        arr_sum = exp_factor * (xv + yv)\n",
    "        arr_exp = np.exp(-arr_sum)\n",
    "        arr_exp[arr_sum > th] = 0\n",
    "        single_keypoint_image[y0:y1 + 1, x0:x1 + 1] = np.maximum(arr_heatmap, arr_exp)\n",
    "        return single_keypoint_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(inputs, input_ch, ouput_ch, is_train):\n",
    "    if input_ch != ouput_ch:\n",
    "        identity = tf.layers.conv2d(inputs, ouput_ch, [1, 1], padding='same')#, activation=tf.nn.relu)\n",
    "        identity = tf.nn.relu(tf.layers.batch_normalization(identity, training=is_train))\n",
    "\n",
    "    else:\n",
    "        identity = inputs\n",
    "\n",
    "    net = tf.layers.conv2d(inputs, input_ch, [1, 1], padding='same')#, activation=tf.nn.relu)\n",
    "    net = tf.nn.relu(tf.layers.batch_normalization(net, training=is_train))\n",
    "\n",
    "    net = tf.layers.conv2d(net, input_ch, [3, 3], padding='same')#, activation=tf.nn.relu)\n",
    "    net = tf.nn.relu(tf.layers.batch_normalization(net, training=is_train))\n",
    "\n",
    "    net = tf.layers.conv2d(net, ouput_ch, [1, 1], padding='same')\n",
    "    net = tf.layers.batch_normalization(net, training=is_train)\n",
    "           \n",
    "    net += identity\n",
    "    return net\n",
    "\n",
    "def pre_hourglass(img, is_train):\n",
    "    save256 = residual(img, 3, 64, is_train) # 256 256 64\n",
    "    save256 = residual(save256, 64, 256, is_train) # 256 256 256\n",
    "    save256 = residual(save256, 256, 256, is_train) # 256 256 256\n",
    " \n",
    "    net = tf.layers.conv2d(img, 64, [7, 7], strides = 2, padding='same')#, activation=tf.nn.relu)\n",
    "    net = tf.nn.relu(tf.layers.batch_normalization(net, training=is_train))\n",
    "\n",
    "  #  print(\"128x128\")\n",
    "  #  print(net.shape)   #128 128 64\n",
    "    \n",
    "    net = residual(net, 64, 128, is_train)\n",
    "    \n",
    "    save128 = residual(net, 128, 256, is_train) # 128 128 64\n",
    "    save128 = residual(save128, 256, 256, is_train) # 128 128 256\n",
    "    save128 = residual(save128, 256, 256, is_train) # 128 128 256\n",
    "    \n",
    "    net = tf.layers.max_pooling2d(net, [2, 2], [2, 2])\n",
    "   # print(\"64x64\") \n",
    "   # print(net.shape) # 64 64 128\n",
    "    \n",
    "    net = residual(net, 128, 256, is_train) # 64 64 256\n",
    "    return net, save256, save128\n",
    "\n",
    "\n",
    "def post_hourglass(net, save256, save128, is_train):      ## input : 64 x 64 x 256\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    \n",
    "    net = tf.image.resize_nearest_neighbor(net, (128,128))\n",
    "    net += save128\n",
    "    \n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    \n",
    "    net = tf.image.resize_nearest_neighbor(net, (256,256))\n",
    "    net += save256\n",
    "    \n",
    "    return net   \n",
    "    \n",
    "def hourglass(net, is_train):#, is_first = True):   ### input : 64 x 64 x 256\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "\n",
    "    save64 = residual(net, 256, 256, is_train) # 64 64 256\n",
    "    save64 = residual(save64, 256, 256, is_train) # 64 64 256\n",
    "    save64 = residual(save64, 256, 256, is_train) # 64 64 256\n",
    "\n",
    "    \n",
    "    net = tf.layers.max_pooling2d(net, [2, 2], [2, 2]) # 32 32 256\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "\n",
    "    save32 = residual(net, 256, 256, is_train) # 32 32 256\n",
    "    save32 = residual(save32, 256, 256, is_train) # 32 32 256\n",
    "    save32 = residual(save32, 256, 256, is_train) # 32 32 256\n",
    "\n",
    "    \n",
    "    net = tf.layers.max_pooling2d(net, [2, 2], [2, 2]) # 16 16 256\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "\n",
    "    save16 = residual(net, 256, 256, is_train) # 16 16 256\n",
    "    save16 = residual(save16, 256, 256, is_train) # 16 16 256\n",
    "    save16 = residual(save16, 256, 256, is_train) # 16 16 256\n",
    "\n",
    "    \n",
    "    net = tf.layers.max_pooling2d(net, [2, 2], [2, 2]) # 8 8 256\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "\n",
    "    \n",
    "    save8 = residual(net, 256, 256, is_train) # 8 8 256\n",
    "    save8 = residual(save8, 256, 256, is_train) # 8 8 256\n",
    "    save8 = residual(save8, 256, 256, is_train) # 8 8 256\n",
    "\n",
    "    \n",
    "    net = tf.layers.max_pooling2d(net, [2, 2], [2, 2]) # 4 4 256\n",
    "    \n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    \n",
    "    \n",
    "    net = tf.image.resize_nearest_neighbor(net, (8,8))\n",
    "    net += save8\n",
    "    \n",
    "    net = residual(net, 256, 256, is_train)\n",
    "\n",
    "    \n",
    "    net = tf.image.resize_nearest_neighbor(net, (16,16))\n",
    "    net += save16\n",
    "    \n",
    "    net = residual(net, 256, 256, is_train)\n",
    "\n",
    "    \n",
    "    net = tf.image.resize_nearest_neighbor(net, (32,32))\n",
    "    net += save32\n",
    "\n",
    "    net = residual(net, 256, 256, is_train)\n",
    "\n",
    "    \n",
    "    net = tf.image.resize_nearest_neighbor(net, (64,64))\n",
    "    net += save64\n",
    "    \n",
    "    net = residual(net, 256, 256, is_train)\n",
    "    \n",
    "    inter_seg = tf.layers.conv2d(net, 15, [1, 1], padding='same')#, activation=tf.nn.relu)  ###input : 256 channel\n",
    "    interpredict_seg = tf.layers.batch_normalization(inter_seg, training=is_train)\n",
    "    \n",
    "    net = residual(interpredict_seg, 15, 128, is_train)\n",
    "    net = residual(net, 128, 256, is_train)\n",
    "    net += save64\n",
    "    \n",
    "    inter_key = tf.layers.conv2d(net, 18, [1, 1], padding='same')#, activation=tf.nn.relu)  ###input : 256 channel\n",
    "    interpredict_key = tf.layers.batch_normalization(inter_key, training=is_train)\n",
    "    \n",
    "    return net, interpredict_key, interpredict_seg   ## 64x64x256,  64x64x33\n",
    "#######################################################\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHGnet(img, numstack, is_train): ##img = [batch, w, h, 3], hourglass-ing\n",
    "    with tf.variable_scope(\"SegNet\"):\n",
    "        net, save256, save128 = pre_hourglass(img, is_train)  ### 256 -> 64\n",
    "\n",
    "        interpredicts_key = []\n",
    "        interpredicts_seg = []\n",
    "        for i in range(numstack):\n",
    "            savenet = net\n",
    "            net, interpredict_key, interpredict_seg = hourglass(net, is_train)#, False)\n",
    "            interpredicts_key.append(interpredict_key)\n",
    "            interpredicts_seg.append(interpredict_seg)\n",
    "\n",
    "            inter_key = tf.layers.conv2d(interpredict_key, 256, [1, 1], padding='same')#, activation=tf.nn.relu)  ###input : 256 channel\n",
    "            inter_key = tf.nn.relu(tf.layers.batch_normalization(inter_key, training=is_train))\n",
    "            inter_seg = tf.layers.conv2d(interpredict_seg, 256, [1, 1], padding='same')#, activation=tf.nn.relu)  ###input : 256 channel\n",
    "            inter_seg = tf.nn.relu(tf.layers.batch_normalization(inter_seg, training=is_train))\n",
    "            \n",
    "            net += inter_key\n",
    "            net += inter_seg\n",
    "            net += savenet\n",
    "        \n",
    "\n",
    "        return interpredicts_key, interpredicts_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(labels, logits, smooth = 2.220446049250313e-16):\n",
    "    product = labels*logits \n",
    "    intersection = tf.reduce_sum(product) \n",
    "    coefficient = (2. * intersection + smooth) / (tf.reduce_sum(logits) + tf.reduce_sum(labels) + smooth) \n",
    "    loss = 1. - tf.reduce_mean(coefficient) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train(save_dir, input_image, joint_gt, seg_gt, joint_predict, seg_predict, step):   #batch result,img\n",
    "    image = input_image[0]\n",
    "    joint_gt = joint_gt[0]\n",
    "    seg_gt = seg_gt[0]\n",
    "    joint_pred = joint_predict[0]\n",
    "    seg_pred = seg_predict[0]\n",
    "\n",
    "    \n",
    "    tmp = joint_predict[0]\n",
    "    \n",
    "   # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "######################joint\n",
    " #   joint_gt = np.argmax(joint_gt,-1)\n",
    "    joint_gt = np.argmax(joint_gt,-1)\n",
    "    \n",
    "    joint_pred = np.argmax(joint_pred,-1)\n",
    "\n",
    "###################seg\n",
    "   # seg_gt = np.argmax(seg_gt,-1)\n",
    "    seg_gt = np.argmax(seg_gt,-1)\n",
    "\n",
    "    \n",
    "  #  seg_pred = np.argmax(seg_pred,-1)\n",
    "    seg_pred = np.argmax(seg_pred,-1)\n",
    "    \n",
    "#     cv2.imwrite(save_dir + '/img'+ str(step) +'_.jpg', image)\n",
    "    \n",
    "#     cv2.imwrite(save_dir + '/img'+ str(step) +'_joint_gt.jpg', joint_gt)\n",
    "#     cv2.imwrite(save_dir + '/img'+ str(step) +'_joint_pred.jpg', joint_pred)   \n",
    "    \n",
    "#     cv2.imwrite(save_dir + '/img'+ str(step) +'_seg_gt.jpg', seg_gt)\n",
    "#     cv2.imwrite(save_dir + '/img'+ str(step) +'_seg_pred.jpg', seg_pred)\n",
    "    \n",
    " #   print(np.unique(joint_gt))\n",
    " #   print(np.unique(seg_gt))\n",
    "\n",
    "    plt.imsave(save_dir + '/img' + str(step) +'_.png', image)#*255)\n",
    "    plt.imsave(save_dir + '/img' + str(step) +'_joint_gt.png', joint_gt)\n",
    "    plt.imsave(save_dir + '/img' + str(step) +'_joint_pred.png', joint_pred)\n",
    "    plt.imsave(save_dir + '/img' + str(step) +'_seg_gt.png', seg_gt)\n",
    "    plt.imsave(save_dir + '/img' + str(step) +'_seg_pred.png', seg_pred)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    tmp = np.amax(tmp[:,:,1:],-1)\n",
    "    tmp *= 255\n",
    "    plt.imsave(save_dir + '/img' + str(step) +'_joint_amax.png', tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-7bec058b1d9e>:6: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From <ipython-input-7-4c6e17ca5420>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/vision/anaconda3/envs/s-core/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-4c6e17ca5420>:4: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-4c6e17ca5420>:38: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From /home/vision/anaconda3/envs/s-core/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph()\n",
    "    ds_train, ds_val = densepose_agu(path_pose_joint)\n",
    "    \n",
    "   # train(ds_train, ds_val)  \n",
    "    iterator = ds_train.make_one_shot_iterator()\n",
    "    one_element = iterator.get_next()\n",
    "    \n",
    "    iterator_val = ds_val.make_one_shot_iterator()\n",
    "    one_element_val = iterator_val.get_next()\n",
    "    \n",
    "    \n",
    "\n",
    "    img_ph = tf.placeholder(tf.float32, [batch_size, image_resize_w, image_resize_h, 3])\n",
    "    joint_gt_ph = tf.placeholder(tf.float32, [batch_size, net_ouput_w, net_ouput_h, 18])\n",
    "    seg_gt_ph = tf.placeholder(tf.float32, [batch_size, net_ouput_w, net_ouput_h, 15])\n",
    "    \n",
    "    is_train = tf.placeholder(tf.bool, shape=())\n",
    "    \n",
    "    predictions_key, predictions_seg = SHGnet(img_ph, nstack, is_train)\n",
    "    \n",
    "    losses_joint = 0\n",
    "    losses_seg = 0\n",
    "    for stack in range(0,nstack):\n",
    "        joint_predict = predictions_key[stack]\n",
    "     #   joint_predict = tf.nn.softmax(joint_predict, -1)\n",
    "        seg_predict = predictions_seg[stack]\n",
    "     #  seg_predict = tf.nn.softmax(seg_predict, -1)\n",
    "\n",
    "\n",
    "     #   loss_joint = dice_loss(joint_gt_ph, joint_predict)\n",
    "     #   loss_seg = dice_loss(seg_gt_ph, seg_predict)\n",
    "        loss_joint = tf.losses.mean_squared_error(joint_gt_ph , joint_predict)\n",
    "        loss_seg = tf.losses.mean_squared_error(seg_gt_ph , seg_predict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        losses_joint += loss_joint\n",
    "        losses_seg += loss_seg\n",
    "    \n",
    "\n",
    "    loss = losses_joint + losses_seg   \n",
    "    loss /= batch_size\n",
    "    loss /= nstack\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = tf.train.AdamOptimizer(0.0005).minimize(loss)\n",
    "    \n",
    "    tf.summary.scalar('loss_joint', loss_joint)\n",
    "    tf.summary.scalar('loss_seg', loss_seg)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vision/anaconda3/envs/s-core/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model/model-400000\n",
      "no pre-trained model\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "step = 0\n",
    "try:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "    with open(save_dir + '/checkpoint') as f:\n",
    "        model_checkpoint_path = f.readline()\n",
    "        global_step = int(re.search(r'\\d+', model_checkpoint_path).group())\n",
    "    step = global_step\n",
    "\n",
    "except:\n",
    "    print(\"no pre-trained model\")   \n",
    "\n",
    "writer_train = tf.summary.FileWriter(\"./log/train\", sess.graph)\n",
    "writer_val = tf.summary.FileWriter(\"./log/val\", sess.graph)\n",
    "\n",
    "print('step', step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    GT_imgNseg = sess.run(one_element)\n",
    "    GT_imgNseg_val = sess.run(one_element_val)\n",
    "    _, train_summary, loss_ = sess.run([train_op, merged, loss], feed_dict={img_ph:GT_imgNseg[0], joint_gt_ph:GT_imgNseg[1], seg_gt_ph:GT_imgNseg[2], is_train:True})\n",
    "    val_summary = sess.run(merged, feed_dict={img_ph:GT_imgNseg_val[0], joint_gt_ph:GT_imgNseg_val[1], seg_gt_ph:GT_imgNseg_val[2], is_train:True})\n",
    "    print(loss_)\n",
    "    writer_train.add_summary(train_summary, step)\n",
    "    writer_train.flush()\n",
    "    writer_val.add_summary(val_summary, step)\n",
    "    writer_val.flush()\n",
    "    step+=1\n",
    "    if step % result_save_freq == 0 or step < 3:\n",
    "        joint_predicted, seg_predicted = sess.run([joint_predict, seg_predict], feed_dict={img_ph:GT_imgNseg[0], joint_gt_ph:GT_imgNseg[1], seg_gt_ph:GT_imgNseg[2], is_train:False})\n",
    "        joint_predicted_val, seg_predicted_val = sess.run([joint_predict, seg_predict], feed_dict={img_ph:GT_imgNseg_val[0], joint_gt_ph:GT_imgNseg_val[1], seg_gt_ph:GT_imgNseg_val[2], is_train:False})\n",
    "\n",
    "        save_train(vis_train_dir, GT_imgNseg[0], GT_imgNseg[3], GT_imgNseg[2], joint_predicted, seg_predicted, step)\n",
    "        save_train(vis_val_dir, GT_imgNseg_val[0], GT_imgNseg_val[3], GT_imgNseg_val[2], joint_predicted_val, seg_predicted_val, step)\n",
    "\n",
    "#          accuracy()\n",
    "\n",
    "        print('step = ', step)\n",
    "    if step % save_freq == 0:\n",
    "        saver.save(sess, os.path.join(save_dir, 'model-' + str(step)))\n",
    "\n",
    "    if step > iterations:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
